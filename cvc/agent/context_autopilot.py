"""
cvc.agent.context_autopilot â€” Self-Healing Context Engine (Context Autopilot).

The #1 problem developers face with AI coding agents in 2025-2026 is
**Context Rot**: after 30-40 turns, the agent forgets instructions, contradicts
itself, re-runs tools it already ran, and hallucinates.

Context Autopilot solves this PROACTIVELY â€” not by waiting until 95% capacity
(like Claude Code) but by continuously monitoring context health and taking
graduated actions at configurable thresholds:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  0-40%   GREEN    âœ¦ Full fidelity â€” no intervention     â”‚
  â”‚ 40-60%   YELLOW   âš¡ Thin old tool outputs â†’ references  â”‚
  â”‚ 60-80%   ORANGE   âš  Smart compact old messages          â”‚
  â”‚ 80-100%  RED      ðŸ”¥ Aggressive compact + auto-commit   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

What makes CVC's approach UNIQUE:
  - Every compaction is preceded by a CVC commit (Merkle DAG snapshot)
  - Zero data loss â€” you can always `cvc restore` to the pre-compacted state
  - Progressive, not binary â€” quality degrades gracefully, not catastrophically
  - Works with ALL 4 providers (Anthropic, OpenAI, Google, Ollama)

Research basis:
  - Anthropic context engineering guide (2025): "treat context as a scarce resource"
  - ContextBranch paper: branching reduces context by 58.1%
  - Letta benchmark: filesystem memory achieves 74.0% accuracy
  - g3 agent: threshold-based thinning at 50/60/70/80%
  - The New Stack (Feb 2026): "Context is AI coding's real bottleneck"
"""

from __future__ import annotations

import logging
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any

logger = logging.getLogger("cvc.autopilot")


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# MODEL CONTEXT LIMITS (tokens)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

MODEL_CONTEXT_LIMITS: dict[str, int] = {
    # Anthropic
    "claude-opus-4-6":       200_000,
    "claude-opus-4-5":       200_000,
    "claude-sonnet-4-6":     200_000,
    "claude-sonnet-4-5":     200_000,
    "claude-haiku-4-5":      200_000,
    # OpenAI
    "gpt-5.3":               128_000,
    "gpt-5.2":               128_000,
    "gpt-5.2-codex":         128_000,
    "gpt-5-mini":            128_000,
    "gpt-4.1":               1_047_576,
    "gpt-4.1-mini":          1_047_576,
    "gpt-4.1-nano":          1_047_576,
    # Google
    "gemini-2.5-flash":      1_048_576,
    "gemini-2.5-pro":        1_048_576,
    "gemini-3-pro-preview":  1_048_576,
    "gemini-3-flash-preview": 1_048_576,
    "gemini-2.5-flash-lite": 1_048_576,
    # Ollama (conservative defaults)
    "qwen2.5-coder:7b":     32_768,
    "qwen3-coder:30b":      32_768,
    "devstral:24b":          32_768,
    "deepseek-r1:8b":        32_768,
}

# Default if model not found
DEFAULT_CONTEXT_LIMIT = 128_000


def get_context_limit(model: str) -> int:
    """Get the context window size for a model."""
    if model in MODEL_CONTEXT_LIMITS:
        return MODEL_CONTEXT_LIMITS[model]
    # Partial match
    for key, limit in MODEL_CONTEXT_LIMITS.items():
        if model.startswith(key):
            return limit
    # Ollama models
    if ":" in model:
        return 32_768
    return DEFAULT_CONTEXT_LIMIT


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# TOKEN ESTIMATION
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def estimate_tokens(text: str) -> int:
    """
    Fast token estimation without requiring tiktoken.

    Uses the empirical ratio of ~1 token per 4 characters for English text
    with code. This is accurate to within ~10% for mixed code/prose, which
    is sufficient for threshold-based decisions.
    """
    if not text:
        return 0
    # Empirical: ~4 chars per token for English prose + code
    # Slightly higher ratio for code (more symbols), lower for prose
    return max(1, len(text) // 4)


def estimate_messages_tokens(messages: list[dict[str, Any]]) -> int:
    """Estimate total tokens in a list of messages (OpenAI format)."""
    total = 0
    for msg in messages:
        # Role token overhead (~4 tokens per message metadata)
        total += 4
        content = msg.get("content", "")
        if isinstance(content, str):
            total += estimate_tokens(content)
        elif isinstance(content, list):
            # Multi-modal content (images, etc.)
            for part in content:
                if isinstance(part, dict):
                    if part.get("type") == "text":
                        total += estimate_tokens(part.get("text", ""))
                    elif part.get("type") == "image_url":
                        total += 1000  # Images are ~1000 tokens
        # Tool calls
        if "tool_calls" in msg:
            for tc in msg.get("tool_calls", []):
                func = tc.get("function", {})
                total += estimate_tokens(func.get("name", ""))
                total += estimate_tokens(func.get("arguments", ""))
    return total


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# HEALTH LEVELS
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

class HealthLevel(str, Enum):
    """Context health level â€” determines the visual indicator and actions."""
    GREEN = "green"      # 0-40% â€” full fidelity
    YELLOW = "yellow"    # 40-60% â€” thinning zone
    ORANGE = "orange"    # 60-80% â€” compaction zone
    RED = "red"          # 80-100% â€” critical zone


@dataclass
class ContextHealthReport:
    """Snapshot of context health at a point in time."""
    estimated_tokens: int
    context_limit: int
    utilization_pct: float
    health_level: HealthLevel
    message_count: int
    tool_result_count: int
    tool_result_tokens: int
    system_tokens: int
    user_tokens: int
    assistant_tokens: int
    actions_taken: list[str] = field(default_factory=list)
    compaction_available: bool = False
    thinning_candidates: int = 0

    @property
    def remaining_tokens(self) -> int:
        return max(0, self.context_limit - self.estimated_tokens)

    @property
    def remaining_pct(self) -> float:
        return max(0.0, 100.0 - self.utilization_pct)

    def format_bar(self, width: int = 20) -> str:
        """Generate a visual health bar like [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 42%."""
        filled = int(self.utilization_pct / 100 * width)
        filled = min(filled, width)
        empty = width - filled
        bar = "â–ˆ" * filled + "â–‘" * empty
        return f"[{bar}] {self.utilization_pct:.0f}%"

    def format_bar_rich(self, width: int = 20) -> str:
        """Generate a Rich-formatted health bar with color."""
        filled = int(self.utilization_pct / 100 * width)
        filled = min(filled, width)
        empty = width - filled

        color_map = {
            HealthLevel.GREEN: "#55AA55",
            HealthLevel.YELLOW: "#CCAA33",
            HealthLevel.ORANGE: "#CC7733",
            HealthLevel.RED: "#FF3333",
        }
        color = color_map.get(self.health_level, "#55AA55")

        bar_filled = "â–ˆ" * filled
        bar_empty = "â–‘" * empty
        return (
            f"[{color}]{bar_filled}[/{color}]"
            f"[#555555]{bar_empty}[/#555555]"
            f" [{color}]{self.utilization_pct:.0f}%[/{color}]"
        )


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# CONTEXT AUTOPILOT
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@dataclass
class AutopilotConfig:
    """Configuration for Context Autopilot thresholds."""
    thin_threshold: float = 0.40        # Start thinning tool outputs at 40%
    compact_threshold: float = 0.60     # Start smart compaction at 60%
    critical_threshold: float = 0.80    # Aggressive compaction at 80%
    enabled: bool = True                # Master switch
    auto_commit_before_compact: bool = True  # CVC commit before compacting
    keep_recent: int = 10               # Messages to keep in compaction
    thin_tool_output_max: int = 500     # Max chars for thinned tool output
    verbose: bool = False               # Log detailed actions


class ContextAutopilot:
    """
    Self-healing context engine that proactively prevents context rot.

    Runs after every turn in the agentic loop, monitors utilization, and
    takes graduated actions to keep the context window healthy.

    The key innovation: CVC commits before every compaction, so developers
    can always restore the full context. No other tool offers this safety.
    """

    def __init__(
        self,
        model: str,
        config: AutopilotConfig | None = None,
    ) -> None:
        self.model = model
        self.config = config or AutopilotConfig()
        self.context_limit = get_context_limit(model)
        self._last_health: ContextHealthReport | None = None
        self._compactions_performed = 0
        self._thinnings_performed = 0
        self._tokens_saved = 0
        self._actions_log: list[dict[str, Any]] = []

    def update_model(self, model: str) -> None:
        """Update the model (and recalculate context limit)."""
        self.model = model
        self.context_limit = get_context_limit(model)

    # â”€â”€ Health Assessment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def assess_health(self, messages: list[dict[str, Any]]) -> ContextHealthReport:
        """
        Assess the current context health without taking any action.
        Returns a detailed health report.
        """
        total_tokens = 0
        tool_result_count = 0
        tool_result_tokens = 0
        system_tokens = 0
        user_tokens = 0
        assistant_tokens = 0

        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, str):
                tokens = estimate_tokens(content)
            elif isinstance(content, list):
                tokens = sum(
                    estimate_tokens(p.get("text", "")) if p.get("type") == "text" else 1000
                    for p in content if isinstance(p, dict)
                )
            else:
                tokens = 0

            tokens += 4  # Message overhead

            role = msg.get("role", "")
            if role == "system":
                system_tokens += tokens
            elif role == "user":
                user_tokens += tokens
            elif role == "assistant":
                assistant_tokens += tokens
            elif role == "tool":
                tool_result_count += 1
                tool_result_tokens += tokens

            total_tokens += tokens

        utilization = (total_tokens / self.context_limit) * 100 if self.context_limit > 0 else 0

        if utilization < self.config.thin_threshold * 100:
            health = HealthLevel.GREEN
        elif utilization < self.config.compact_threshold * 100:
            health = HealthLevel.YELLOW
        elif utilization < self.config.critical_threshold * 100:
            health = HealthLevel.ORANGE
        else:
            health = HealthLevel.RED

        # Count thinning candidates (tool results > threshold)
        thinning_candidates = 0
        for msg in messages:
            if msg.get("role") == "tool":
                content = msg.get("content", "")
                if isinstance(content, str) and len(content) > self.config.thin_tool_output_max * 2:
                    thinning_candidates += 1

        report = ContextHealthReport(
            estimated_tokens=total_tokens,
            context_limit=self.context_limit,
            utilization_pct=round(utilization, 1),
            health_level=health,
            message_count=len(messages),
            tool_result_count=tool_result_count,
            tool_result_tokens=tool_result_tokens,
            system_tokens=system_tokens,
            user_tokens=user_tokens,
            assistant_tokens=assistant_tokens,
            thinning_candidates=thinning_candidates,
            compaction_available=len(messages) > self.config.keep_recent + 3,
        )

        self._last_health = report
        return report

    # â”€â”€ Context Thinning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def thin_tool_outputs(
        self,
        messages: list[dict[str, Any]],
        keep_recent_n: int = 6,
    ) -> tuple[list[dict[str, Any]], int]:
        """
        Replace large tool outputs in older messages with compact references.

        Preserves the N most recent messages untouched. Only thins tool
        messages that are older and exceed the size threshold.

        Returns (modified_messages, tokens_saved).
        """
        if len(messages) <= keep_recent_n:
            return messages, 0

        tokens_before = estimate_messages_tokens(messages)
        result = []
        thinned_count = 0

        cutoff = len(messages) - keep_recent_n

        for i, msg in enumerate(messages):
            if i < cutoff and msg.get("role") == "tool":
                content = msg.get("content", "")
                if isinstance(content, str) and len(content) > self.config.thin_tool_output_max * 2:
                    # Extract tool name and first line for reference
                    tool_name = msg.get("name", "tool")
                    first_line = content.split("\n")[0][:100]
                    char_count = len(content)

                    # Create thinned version
                    thinned_msg = dict(msg)
                    thinned_msg["content"] = (
                        f"[Thinned by Context Autopilot] {tool_name}: "
                        f"{first_line}â€¦ ({char_count:,} chars â†’ reference)"
                    )
                    result.append(thinned_msg)
                    thinned_count += 1
                    continue

            result.append(msg)

        tokens_after = estimate_messages_tokens(result)
        tokens_saved = max(0, tokens_before - tokens_after)

        if thinned_count > 0:
            self._thinnings_performed += thinned_count
            self._tokens_saved += tokens_saved
            logger.info(
                "Context Autopilot: thinned %d tool outputs, saved ~%d tokens",
                thinned_count, tokens_saved,
            )

        return result, tokens_saved

    # â”€â”€ Smart Compaction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def smart_compact_messages(
        self,
        messages: list[dict[str, Any]],
        keep_recent: int | None = None,
    ) -> tuple[list[dict[str, Any]], int]:
        """
        Intelligently compact the message history.

        Strategy:
          1. Always keep the system prompt (messages[0])
          2. Keep the last `keep_recent` messages verbatim
          3. For older messages:
             - Keep "important" messages (code blocks, decisions, errors)
             - Summarize the rest into compact blurbs
          4. Add a compaction header

        Returns (compacted_messages, messages_removed).
        """
        keep_n = keep_recent or self.config.keep_recent

        if len(messages) <= keep_n + 2:
            return messages, 0

        # Split: system prompt + older + recent
        system_msgs = []
        conversation = []

        for msg in messages:
            if msg.get("role") == "system" and not conversation:
                system_msgs.append(msg)
            else:
                conversation.append(msg)

        if len(conversation) <= keep_n:
            return messages, 0

        older = conversation[:-keep_n]
        recent = conversation[-keep_n:]

        # Classify older messages
        important: list[dict[str, Any]] = []
        compressible: list[dict[str, Any]] = []

        KEY_TERMS = {
            "decision", "decided", "architecture", "design", "important",
            "critical", "error", "fix", "bug", "security", "api", "schema",
            "database", "migration", "deploy", "config", "breaking",
            "requirement", "must", "trade-off", "conclusion", "solution",
            "approach", "strategy", "pattern", "warning", "failed",
        }

        for msg in older:
            content = msg.get("content", "")
            if not isinstance(content, str):
                compressible.append(msg)
                continue

            is_important = False
            role = msg.get("role", "")

            # System messages are always important
            if role == "system":
                is_important = True
            # Messages with code blocks
            elif "```" in content:
                is_important = True
            # Messages with key decision terms
            elif any(term in content.lower() for term in KEY_TERMS):
                is_important = True
            # Very short messages (questions, confirmations)
            elif len(content) < 100:
                is_important = True

            if is_important:
                # But truncate long important messages
                if len(content) > 1000:
                    msg = dict(msg)
                    msg["content"] = content[:1000] + "\nâ€¦[truncated by Context Autopilot]"
                important.append(msg)
            else:
                compressible.append(msg)

        # Summarize compressible messages in chunks
        summaries: list[dict[str, Any]] = []
        chunk_size = 5

        for i in range(0, len(compressible), chunk_size):
            chunk = compressible[i:i + chunk_size]
            points = []
            for m in chunk:
                role = m.get("role", "?")
                content = m.get("content", "")
                if isinstance(content, str):
                    first_line = content.split("\n")[0][:120]
                    points.append(f"  â€¢ [{role}] {first_line}")

            if points:
                summary_text = (
                    f"[Context Autopilot] Summary of {len(chunk)} messages:\n"
                    + "\n".join(points)
                )
                summaries.append({"role": "system", "content": summary_text})

        # Build the compacted result
        original_count = len(messages)
        compaction_header = {
            "role": "system",
            "content": (
                f"[Context Autopilot] Auto-compacted: {original_count} â†’ "
                f"{len(system_msgs) + 1 + len(important) + len(summaries) + len(recent)} messages. "
                f"{len(compressible)} messages summarized, {len(important)} important preserved. "
                f"Use '/restore' to recover full history from CVC."
            ),
        }

        compacted = (
            system_msgs
            + [compaction_header]
            + important
            + summaries
            + recent
        )

        removed = original_count - len(compacted)
        self._compactions_performed += 1

        logger.info(
            "Context Autopilot: compacted %d â†’ %d messages (removed %d)",
            original_count, len(compacted), removed,
        )

        return compacted, removed

    # â”€â”€ Main Autopilot Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def run(
        self,
        messages: list[dict[str, Any]],
        engine: Any = None,
    ) -> tuple[list[dict[str, Any]], ContextHealthReport]:
        """
        Run the Context Autopilot on the current message list.

        This is called after every turn in the agentic loop. It:
          1. Assesses context health
          2. Takes graduated actions based on thresholds
          3. Returns the (possibly modified) message list + health report

        If `engine` is provided (CVCEngine), auto-commits before compaction.

        Returns (messages, health_report).
        """
        if not self.config.enabled:
            report = self.assess_health(messages)
            return messages, report

        report = self.assess_health(messages)

        if report.health_level == HealthLevel.GREEN:
            # All good â€” no intervention needed
            return messages, report

        actions: list[str] = []

        # â”€â”€ YELLOW ZONE (40-60%): Thin old tool outputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if report.health_level in (HealthLevel.YELLOW, HealthLevel.ORANGE, HealthLevel.RED):
            if report.thinning_candidates > 0:
                messages, tokens_saved = self.thin_tool_outputs(messages)
                if tokens_saved > 0:
                    actions.append(
                        f"Thinned {report.thinning_candidates} tool outputs "
                        f"(saved ~{tokens_saved:,} tokens)"
                    )

        # â”€â”€ ORANGE ZONE (60-80%): Smart compaction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if report.health_level in (HealthLevel.ORANGE, HealthLevel.RED):
            if report.compaction_available:
                # Auto-commit before compaction (CVC safety net)
                if self.config.auto_commit_before_compact and engine is not None:
                    try:
                        from cvc.core.models import CVCCommitRequest
                        pre_compact_result = engine.commit(
                            CVCCommitRequest(
                                message=(
                                    f"Pre-compact checkpoint "
                                    f"({report.utilization_pct:.0f}% context utilization)"
                                ),
                                tags=["autopilot", "pre-compact"],
                            )
                        )
                        if pre_compact_result.success:
                            actions.append(
                                f"Auto-committed pre-compact snapshot: "
                                f"{pre_compact_result.commit_hash[:12] if pre_compact_result.commit_hash else '?'}"
                            )
                    except Exception as e:
                        logger.warning("Autopilot pre-compact commit failed: %s", e)

                # Perform compaction
                keep = self.config.keep_recent
                if report.health_level == HealthLevel.RED:
                    # Critical: keep fewer messages for stronger compression
                    keep = max(6, keep - 4)

                messages, removed = self.smart_compact_messages(messages, keep_recent=keep)
                if removed > 0:
                    actions.append(f"Smart-compacted {removed} messages")

        # â”€â”€ RED ZONE (80-100%): Additional aggressive measures â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if report.health_level == HealthLevel.RED:
            # Do a second thinning pass with stricter thresholds
            original_threshold = self.config.thin_tool_output_max
            self.config.thin_tool_output_max = 200
            messages, extra_saved = self.thin_tool_outputs(messages, keep_recent_n=4)
            self.config.thin_tool_output_max = original_threshold
            if extra_saved > 0:
                actions.append(f"Aggressive thinning saved ~{extra_saved:,} more tokens")

        # Re-assess after actions
        if actions:
            report = self.assess_health(messages)
            report.actions_taken = actions
            self._actions_log.append({
                "timestamp": time.time(),
                "actions": actions,
                "before_pct": self._last_health.utilization_pct if self._last_health else 0,
                "after_pct": report.utilization_pct,
            })

        return messages, report

    # â”€â”€ Diagnostics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_diagnostics(self) -> dict[str, Any]:
        """Get detailed autopilot diagnostics for /health command."""
        return {
            "enabled": self.config.enabled,
            "model": self.model,
            "context_limit": self.context_limit,
            "thresholds": {
                "thin": f"{self.config.thin_threshold * 100:.0f}%",
                "compact": f"{self.config.compact_threshold * 100:.0f}%",
                "critical": f"{self.config.critical_threshold * 100:.0f}%",
            },
            "session_stats": {
                "compactions_performed": self._compactions_performed,
                "thinnings_performed": self._thinnings_performed,
                "tokens_saved": self._tokens_saved,
                "actions_log": self._actions_log[-10:],  # Last 10 actions
            },
            "last_health": {
                "utilization": f"{self._last_health.utilization_pct:.1f}%" if self._last_health else "N/A",
                "health_level": self._last_health.health_level.value if self._last_health else "unknown",
                "messages": self._last_health.message_count if self._last_health else 0,
                "tokens": self._last_health.estimated_tokens if self._last_health else 0,
            },
        }

    @property
    def last_health(self) -> ContextHealthReport | None:
        return self._last_health
